# Vocalis - Quick Start Guide

Guide pour lancer le projet Vocalis en dÃ©veloppement et production.

## ğŸ“‹ PrÃ©requis

### Backend
- Python 3.11+
- pip (gestionnaire de paquets Python)
- Connexion internet (pour tÃ©lÃ©charger les dÃ©pendances)

### Frontend
- Flutter SDK 3.11+
- Dart 3.11+
- Un navigateur moderne (pour web) ou un Ã©mulateur/device (pour mobile)

### ModÃ¨le LLM
- TinyLlama 1.1B GGUF (~2GB tÃ©lÃ©chargÃ©, ~1.5GB utilisÃ©)
- AccÃ¨s Ã  Ollama (optionnel, voir section Ollama)

---

## ğŸš€ Lancement Rapide (Local)

### Ã‰tape 1: Lancer le Backend

```bash
# Naviguer vers le rÃ©pertoire backend
cd backend

# CrÃ©er et activer l'environnement virtuel
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# ou: venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# TÃ©lÃ©charger le modÃ¨le TinyLlama (si pas dÃ©jÃ  prÃ©sent)
# Voir section "TÃ©lÃ©charger le ModÃ¨le LLM" ci-dessous

# Lancer le serveur
python main.py
```

**RÃ©sultat attendu:**
```
INFO:vocalis-backend:Loading model from backend/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf...
INFO:vocalis-backend:Model loaded successfully!
INFO uvicorn.server - Application startup complete [production mode]
```

Le backend est maintenant accessible sur `http://localhost:8080`

### Ã‰tape 2: Lancer le Frontend (Web)

Dans un **nouveau terminal**:

```bash
# Naviguer vers le rÃ©pertoire frontend
cd frontend

# Obtenir les dÃ©pendances Flutter
flutter pub get

# Lancer en mode web (pointant vers localhost:8080)
flutter run -d chrome
```

**RÃ©sultat attendu:**
- Une fenÃªtre Chrome s'ouvre automatiquement
- Application Vocalis accessible sur `http://localhost:53781` (port variable)

---

## ğŸ¯ AccÃ¨s Ã  l'Application

Une fois le backend et frontend lancÃ©s:

1. **Ouvrir l'application** (URL affichÃ©e dans le terminal Flutter)
2. **Voir le message de bienvenue** "Bonjour! Pour rÃ©diger une ordonnance..."
3. **Commencer Ã  converser** pour collecter les informations du patient

### Workflow d'Utilisation

1. **Collecte d'informations** (chat)
   - Entrez les informations du patient
   - L'IA demande les champs manquants
   - Le bouton "âœ“" apparaÃ®t quand toutes les infos sont collectÃ©es

2. **GÃ©nÃ©ration d'ordonnance**
   - Cliquez sur "âœ“" pour gÃ©nÃ©rer l'ordonnance
   - Passez Ã  l'Ã©cran de rÃ©vision

3. **RÃ©vision et Ã©dition**
   - Lisez l'ordonnance gÃ©nÃ©rÃ©e
   - Ã‰ditez si nÃ©cessaire
   - Cliquez "Proceed to Signature"

4. **Signature et PDF**
   - Signez sur le pad
   - Cliquez "Generate & Download PDF"
   - Le PDF est gÃ©nÃ©rÃ© et tÃ©lÃ©chargÃ©

---

## ğŸ“± Lancer sur DiffÃ©rentes Plateformes

### Web (Chrome/Firefox/Safari)

```bash
cd frontend
flutter run -d chrome     # Chrome (recommandÃ©)
flutter run -d firefox    # Firefox
# Pour Safari: ouvrir dans Xcode
```

### Android (Ã‰mulateur)

```bash
# DÃ©marrer un Ã©mulateur Android depuis Android Studio
# Ou: emulator -avd <avd_name>

cd frontend
flutter run -d android-emulator
```

### iOS (Simulateur Mac)

```bash
# DÃ©marrer le simulateur iOS
open /Applications/Xcode.app/Contents/Developer/Applications/Simulator.app

cd frontend
flutter run -d ios
```

### macOS/Linux/Windows (Desktop)

```bash
cd frontend

# macOS
flutter run -d macos

# Linux
flutter run -d linux

# Windows
flutter run -d windows
```

---

## ğŸ³ Lancer avec Docker (Optionnel)

### Backend avec Docker

```bash
# Construire l'image
docker build -t vocalis-backend ./backend

# Lancer le container
docker run -p 8080:8080 \
  -v $(pwd)/backend/models:/app/models \
  vocalis-backend
```

---

## ğŸ§  TÃ©lÃ©charger le ModÃ¨le LLM

Le modÃ¨le TinyLlama est requis pour le fonctionnement du backend.

### Option 1: TÃ©lÃ©chargement Manuel

```bash
cd backend/models

# TÃ©lÃ©charger depuis Hugging Face
# Taille: ~2GB
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Ou avec curl
curl -L -o tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
```

### Option 2: Avec Ollama

Ollama simplifie le tÃ©lÃ©chargement et la gestion des modÃ¨les.

```bash
# Installer Ollama (https://ollama.ai)

# TÃ©lÃ©charger le modÃ¨le
ollama pull tinyllama

# Le modÃ¨le est maintenant disponible via Ollama
# Voir: backend/ollama_setup.sh pour l'intÃ©gration
```

---

## ğŸ”§ Configuration AvancÃ©e

### Variables d'Environnement Backend

```bash
# Chemin du modÃ¨le personnalisÃ©
export MODEL_PATH=/chemin/vers/modele.gguf

# Timeout pour les rÃ©ponses (secondes)
export OLLAMA_TIMEOUT=120

# Port du backend
export BACKEND_PORT=8080

# Lancer le backend
python main.py
```

### Configuration Frontend pour Production

Pour dÃ©ployer le frontend en production:

```bash
# Build web pour production (example avec API externe)
flutter build web --dart-define=API_URL=https://api.example.com:8080 --release

# RÃ©sultat dans: frontend/build/web/
# DÃ©ployer les fichiers sur un serveur web (Nginx, Apache, etc.)
```

---

## ğŸ§ª Tester les Endpoints

### Via curl

```bash
# Health check
curl http://localhost:8080/api/health

# Chat simple
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour"}'

# Collecter des infos
curl -X POST http://localhost:8080/api/collect-prescription-info \
  -H "Content-Type: application/json" \
  -d '{
    "currentData": {},
    "userInput": "Patient Jean Dupont, 45 ans"
  }'
```

Pour plus d'exemples: voir `backend/API_TEST_EXAMPLES.md`

### Via Tests AutomatisÃ©s

```bash
cd backend

# Tous les tests
pytest test_main.py test_advanced.py -v

# Tests spÃ©cifiques
pytest test_main.py::TestPrescriptionDataModel -v
```

---

## ğŸ› DÃ©pannage

### Le modÃ¨le ne charge pas

**SymptÃ´me:** `Failed to load model` au dÃ©marrage du backend

**Solutions:**
1. VÃ©rifier que le fichier existe: `backend/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf`
2. VÃ©rifier les permissions: `ls -l backend/models/`
3. Augmenter le timeout dans le code ou via `OLLAMA_TIMEOUT`
4. RÃ©essayer avec le modÃ¨le tÃ©lÃ©chargÃ© via Ollama

### Frontend ne peut pas se connecter au backend

**SymptÃ´me:** Erreur "Failed to connect" ou "Connection refused"

**Solutions:**
1. VÃ©rifier que le backend tourne: `curl http://localhost:8080/api/health`
2. VÃ©rifier les ports (backend: 8080, frontend: variable)
3. VÃ©rifier la configuration API dans `frontend/lib/api_service.dart`
4. Sur web, vÃ©rifier les CORS du backend (dÃ©jÃ  configurÃ©s)

### Flutter build Ã©choue

**Solutions:**
```bash
# Nettoyer et reconstruire
flutter clean
flutter pub get
flutter run -d chrome

# Ou en mode debug verbose
flutter run -d chrome -v
```

### Erreur "Module 'llama_cpp' not found"

```bash
# RÃ©installer les dÃ©pendances
cd backend
pip install --upgrade -r requirements.txt
```

---

## ğŸ“Š Architecture du SystÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Vocalis Application                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Frontend (Flutter) â”‚   Backend (FastAPI)          â”‚
â”‚                      â”‚                              â”‚
â”‚ â€¢ Web (Chrome)       â”‚ â€¢ REST API                   â”‚
â”‚ â€¢ iOS/Android        â”‚ â€¢ TinyLlama LLM              â”‚
â”‚ â€¢ macOS/Linux        â”‚ â€¢ PDF Generation             â”‚
â”‚ â€¢ Windows            â”‚ â€¢ Data Validation            â”‚
â”‚                      â”‚                              â”‚
â”‚   Ports:             â”‚   Port: 8080                 â”‚
â”‚   â€¢ Web: ~53781      â”‚   â€¢ http://localhost:8080    â”‚
â”‚   â€¢ Mobile: Device   â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Communication: HTTP REST (JSON)
CORS: Enabled for all origins
Authentication: None (local/trusted network)
```

---

## ğŸ“ˆ Performance

### Temps de DÃ©marrage

| Composant | Temps |
|-----------|-------|
| Backend (sans modÃ¨le) | ~2s |
| Chargement modÃ¨le TinyLlama | ~30-60s (1Ã¨re fois) |
| Frontend (web) | ~5-10s |
| **Total (1Ã¨re fois)** | **~1-2 minutes** |

### Temps de RÃ©ponse (Avec ModÃ¨le ChargÃ©)

| Action | Temps |
|--------|-------|
| Chat simple | ~2-5s |
| Collecte info | ~2-5s |
| GÃ©nÃ©ration ordonnance | ~3-8s |
| GÃ©nÃ©ration PDF | ~1-2s |

---

## ğŸ” SÃ©curitÃ© en DÃ©veloppement

âš ï¸ **Important pour le dÃ©veloppement local uniquement:**

- CORS: Accepte toutes les origines
- Authentication: Aucune (supposant un rÃ©seau de confiance)
- API: HTTP sans TLS
- ModÃ¨le: ChargÃ© en mÃ©moire

**Pour la production:**
- Ajouter l'authentification (JWT, OAuth, etc.)
- Utiliser HTTPS/TLS
- Configurer CORS appropriÃ©
- Ajouter rate limiting
- Valider toutes les entrÃ©es

---

## ğŸ“š Documentation SupplÃ©mentaire

- **CLAUDE.md** - DÃ©cisions d'architecture et configuration
- **backend/TEST_REPORT.md** - RÃ©sultats des tests
- **backend/API_TEST_EXAMPLES.md** - Exemples API curl
- **frontend/README.md** - Instructions Flutter

---

## ğŸš€ DÃ©ploiement en Production

### Backend

```bash
# Build production
pip install gunicorn

# Lancer avec Gunicorn (production-ready)
gunicorn -w 4 -b 0.0.0.0:8080 main:app
```

### Frontend

```bash
# Build web production
flutter build web --dart-define=API_URL=https://api.example.com:8080 --release

# DÃ©ployer sur Vercel, Netlify, or AWS S3 + CloudFront
# Copier le contenu de build/web/ vers votre serveur
```

---

## âœ… Checklist de Lancement

- [ ] Python 3.11+ installÃ©
- [ ] Flutter SDK installÃ©
- [ ] ModÃ¨le TinyLlama tÃ©lÃ©chargÃ© (2GB)
- [ ] DÃ©pendances backend installÃ©es (`pip install -r requirements.txt`)
- [ ] DÃ©pendances frontend installÃ©es (`flutter pub get`)
- [ ] Backend lancÃ© (`python main.py`)
- [ ] Frontend lancÃ© (`flutter run -d chrome`)
- [ ] Application accessible et fonctionnelle
- [ ] Tests passent (`pytest test_main.py test_advanced.py -v`)

---

## ğŸ’¡ Tips & Tricks

### AccÃ©lÃ©rer le dÃ©veloppement

```bash
# Frontend: Hot reload automatique
flutter run -d chrome --fast-start

# Backend: Auto-reload avec watchdog
pip install watchdog
python main.py  # DÃ©jÃ  configurÃ© avec reload=True
```

### DÃ©boguer les requÃªtes API

```bash
# Voir les requÃªtes HTTP dÃ©taillÃ©es
curl -v http://localhost:8080/api/health

# Ou dans le code frontend:
# Activer les logs dans ApiService
```

### RÃ©initialiser tout

```bash
# Backend
cd backend
rm -rf venv __pycache__ *.pyc
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Frontend
cd frontend
flutter clean
flutter pub get
```

---

**Besoin d'aide?** Consultez les fichiers de documentation ou rÃ©exÃ©cutez les tests.

**Status**: âœ… PrÃªt Ã  lancer!

---

Generated: 2026-02-15
